{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ac151b-3b3a-4238-9a5b-3e5dca122326",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdea92-a01b-4f20-baa7-e5495f1f63c3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab86fdf-de40-4796-b00c-d4da3e65bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from delta import *\n",
    "from pyspark.sql.functions import from_json, col, floor, window, concat_ws\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c90f46d-8d4f-4609-b662-2d9ed30574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession with the Kafka JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTaxiStream\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a407f2b8-4c63-412c-9c90-8a25f05655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema for Incoming Data\n",
    "schema = StructType() \\\n",
    "    .add(\"medallion\", StringType()) \\\n",
    "    .add(\"hack_license\", StringType()) \\\n",
    "    .add(\"pickup_datetime\", TimestampType()) \\\n",
    "    .add(\"dropoff_datetime\", TimestampType()) \\\n",
    "    .add(\"trip_time_in_secs\", DoubleType()) \\\n",
    "    .add(\"trip_distance\", DoubleType()) \\\n",
    "    .add(\"pickup_longitude\", DoubleType()) \\\n",
    "    .add(\"pickup_latitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_longitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_latitude\", DoubleType()) \\\n",
    "    .add(\"payment_type\", StringType()) \\\n",
    "    .add(\"fare_amount\", DoubleType()) \\\n",
    "    .add(\"surcharge\", DoubleType()) \\\n",
    "    .add(\"mta_tax\", DoubleType()) \\\n",
    "    .add(\"tip_amount\", DoubleType()) \\\n",
    "    .add(\"tolls_amount\", DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kafka stream\n",
    "taxi_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"taxi-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "parsed_taxi_stream = taxi_stream.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bc510-4ddc-45ed-aaf1-8a33fd614d03",
   "metadata": {},
   "source": [
    "# Query 0: Data Cleansing and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1dd0a5-68d2-45c0-8842-1768f91c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove malformed and invalid data\n",
    "cleaned_taxi_stream = parsed_taxi_stream \\\n",
    "    .filter(\"medallion IS NOT NULL AND hack_license IS NOT NULL\") \\\n",
    "    .filter(\"pickup_longitude != 0.0 AND pickup_latitude != 0.0\") \\\n",
    "    .filter(\"dropoff_longitude != 0.0 AND dropoff_latitude != 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5968d8ef-d0e3-420d-9f67-256e35d05ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cleaned_taxi_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"taxi_trips_cleaned\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4abc85-cefc-40c3-a1d6-95f6ba11ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|medallion|hack_license|pickup_datetime|dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View cleansed stream in notebook\n",
    "time.sleep(1)\n",
    "spark.sql(\"SELECT * FROM taxi_trips_cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555b6a1-5a5c-4d2c-b392-79977f2b3c89",
   "metadata": {},
   "source": [
    "# Query 1: Frequent Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f88189-8394-463e-9114-c6aad07b6c75",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37e0f61-17e8-4e07-a543-fec9aa875975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ba2a62-05a0-46d4-9617-a9876b8e08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_lat = 41.474937\n",
    "reference_lon = -74.913585\n",
    "total_cells = 300\n",
    "\n",
    "# Cell sizes from http://www.debs2015.org/call-grand-challenge.html\n",
    "cell_size_lat_deg = 0.004491556  # 500m south\n",
    "cell_size_lon_deg = 0.005986     # 500m east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cc6b1a-4ad4-4e4d-a7c8-caf0d2c6a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to calculate how far the cell location is from the origin\n",
    "def get_cell_id(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    try:\n",
    "        dx = int((lon - reference_lon) / cell_size_lon_deg) + 1 # how many cells east\n",
    "        dy = int((reference_lat - lat) / cell_size_lat_deg) + 1 # how many cells south\n",
    "        if 1 <= dx <= total_cells and 1 <= dy <= total_cells: # validate\n",
    "            return f\"{dx}.{dy}\"\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# create spark udf\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c843e763-11a8-4e28-80b7-9da7334fcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take cleaned taxi stream data and convert pickup and dropoff locations into start and end cell IDs\n",
    "stream_with_cells = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"end_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL AND end_cell_id IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ea44ef-8517-4a7f-842b-266c1768f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rides for each route in the last 30 minutes \n",
    "frequent_routes = stream_with_cells \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"30 minutes\"),\n",
    "        col(\"start_cell_id\"),\n",
    "        col(\"end_cell_id\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\"start_cell_id\", \"end_cell_id\", \"count\") \\\n",
    "    .orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3219b4-8d30-4041-83df-2e24cb3df4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f6b8ade6ad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store results\n",
    "frequent_routes.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"top_routes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01aa8050-4b84-42e4-ba22-467f860def02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+\n",
      "|start_cell_id|end_cell_id|count|\n",
      "+-------------+-----------+-----+\n",
      "+-------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time.sleep(15)\n",
    "spark.sql(\"SELECT * FROM top_routes LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9299a-ce45-455c-a4a6-311f47ac8c48",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138b680-2314-4517-8694-9e2013bf7205",
   "metadata": {},
   "source": [
    "# Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71532313-5144-4e9f-b28d-8c55da28057d",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52959a9c-7791-4604-a655-bcf5cc28e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import window, max as max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3425dc-2fc8-4b67-977f-3d5816f61051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide query 1 degrees by 2\n",
    "cell_size_lat_deg = 0.002245778   # 250m south\n",
    "cell_size_lon_deg = 0.002993      # 250m east\n",
    "total_cells = 600\n",
    "\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c53427f1-d852-4a83-9b2f-1a2b5ba774f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, window\n",
    "\n",
    "# Add profit and cell columns\n",
    "taxi_stream_v2 = cleaned_taxi_stream \\\n",
    "    .withColumn(\"pickup_time\", col(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_time\", col(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "    .withColumn(\"pickup_cell\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"dropoff_cell\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35615084-9368-447d-8a7e-f4f77892c1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, window, unix_timestamp\n",
    "\n",
    "# Watermarked dropoffs and pickups\n",
    "dropoffs = taxi_stream_v2 \\\n",
    "    .filter(\"dropoff_cell IS NOT NULL\") \\\n",
    "    .withWatermark(\"dropoff_time\", \"30 minutes\") \\\n",
    "    .selectExpr(\"medallion AS medallion_drop\", \"dropoff_time\", \"dropoff_cell\")\n",
    "\n",
    "pickups = taxi_stream_v2 \\\n",
    "    .filter(\"pickup_cell IS NOT NULL\") \\\n",
    "    .withWatermark(\"pickup_time\", \"30 minutes\") \\\n",
    "    .selectExpr(\"medallion AS medallion_pick\", \"pickup_time\")\n",
    "\n",
    "\n",
    "# Left join to simulate left_anti\n",
    "joined_dropoffs = dropoffs.join(\n",
    "    pickups,\n",
    "    expr(\"\"\"\n",
    "        medallion_drop = medallion_pick AND\n",
    "        pickup_time > dropoff_time AND\n",
    "        pickup_time <= dropoff_time + interval 30 minutes\n",
    "    \"\"\"),\n",
    "    \"leftOuter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb69523-bce5-4605-8063-ca6350e49a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing dropoffs to memory\n",
    "dropoffs_query = dropoffs.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"dropoffs\") \\\n",
    "    .start()\n",
    "\n",
    "# Writing pickups to memory\n",
    "pickups_query = pickups.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"pickups\") \\\n",
    "    .start()\n",
    "\n",
    "# Print the results\n",
    "joined_query = joined_dropoffs.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"joined\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e0c664a-19a7-4d54-8990-281fe742a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_dropoffs = joined_dropoffs \\\n",
    "    .filter((unix_timestamp(current_timestamp()) - unix_timestamp(\"dropoff_time\") <= 1800) & col(\"pickup_time\").isNull()) \\\n",
    "    .select(\"dropoff_time\", \"dropoff_cell\")\n",
    "\n",
    "# Step 2: Group by dropoff cell and apply window to count the empty taxis\n",
    "empty_taxi_count = recent_dropoffs \\\n",
    "    .groupBy(\n",
    "        window(\"dropoff_time\", \"15 minutes\", \"1 minute\"),  # 15-minute window with 1-minute slide\n",
    "        col(\"dropoff_cell\").alias(\"cell_id\")\n",
    "    ) \\\n",
    "    .agg(expr(\"count(*) AS empty_taxis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47fda0e6-547e-4b27-a3be-f5f4138a0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_dropoffs = recent_dropoffs.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"recent_dropoffs\") \\\n",
    "    .start()\n",
    "\n",
    "empty_taxi_count_query = empty_taxi_count.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"empty_taxi_count\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deed42c0-7394-4038-bf5f-9057f7ba260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|dropoff_time|dropoff_cell|\n",
      "+------------+------------+\n",
      "+------------+------------+\n",
      "\n",
      "+------+-------+-----------+\n",
      "|window|cell_id|empty_taxis|\n",
      "+------+-------+-----------+\n",
      "+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c25944a1-b871-432a-b3bf-9b7dc42f4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_stream = taxi_stream_v2 \\\n",
    "    .filter(\"pickup_cell IS NOT NULL\") \\\n",
    "    .withWatermark(\"dropoff_time\", \"20 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(\"dropoff_time\", \"15 minutes\", \"1 minute\"),\n",
    "        col(\"pickup_cell\").alias(\"cell_id\")\n",
    "    ).agg(expr(\"percentile_approx(profit, 0.5) AS median_profit\"))\n",
    "\n",
    "# Join and FLATTEN the window for Delta\n",
    "profitability_stream = profit_stream.join(\n",
    "    empty_taxi_count,\n",
    "    [\"window\", \"cell_id\"]\n",
    ").withColumn(\n",
    "    \"profitability\", col(\"median_profit\") / col(\"empty_taxis\")\n",
    ").select(\n",
    "    col(\"window.start\").cast(\"timestamp\").alias(\"pickup_datetime\"),\n",
    "    col(\"window.end\").cast(\"timestamp\").alias(\"dropoff_datetime\"),\n",
    "    \"cell_id\",\n",
    "    \"empty_taxis\",\n",
    "    \"median_profit\",\n",
    "    \"profitability\"\n",
    ")\n",
    "\n",
    "def write_batch_to_delta(batch_df, batch_id):\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"[Batch {batch_id}] ⛔ Skipped empty batch\")\n",
    "        return\n",
    "\n",
    "    # Only write if DataFrame has records\n",
    "    batch_df.select(\n",
    "        col(\"window.start\").alias(\"pickup_datetime\"),\n",
    "        col(\"window.end\").alias(\"dropoff_datetime\"),\n",
    "        \"cell_id\",\n",
    "        \"empty_taxis\",\n",
    "        \"median_profit\",\n",
    "        \"profitability\"\n",
    "    ).write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .save(\"./output/profitable_areas_delta\")\n",
    "\n",
    "\n",
    "# Start the stream\n",
    "query = profitability_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/profitable_areas\") \\\n",
    "    .foreachBatch(write_batch_to_delta) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd31c11-69cd-4557-a6fb-efe1e5807f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Initializing sources',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.isActive  # Should return True\n",
    "query.lastProgress  # Shows the most recent batch info (after one has run)\n",
    "query.status  # Will tell you \"Waiting for data\" or \"Processing new data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96603134-85da-428c-9c44-5431cc0962e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n",
      "|pickup_latitude|pickup_longitude|\n",
      "+---------------+----------------+\n",
      "+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT pickup_latitude, pickup_longitude\n",
    "    FROM taxi_trips_cleaned\n",
    "    WHERE pickup_latitude IS NOT NULL AND pickup_longitude IS NOT NULL\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3de13125-1ad6-4ed4-a537-27aa2a46e94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f6c1d1e32d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropoffs.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "pickups.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd039f4e-ea57-4bb9-9e1b-e816ca2a7005",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: output/profitable_areas_delta.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./output/profitable_areas_delta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofitability\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: output/profitable_areas_delta."
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"./output/profitable_areas_delta\")\n",
    "df.orderBy(\"profitability\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f599be-0dac-468e-a880-c02d5c553b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS profitable_areas\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE profitable_areas\n",
    "    USING DELTA\n",
    "    LOCATION './output/profitable_areas_delta'\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT * FROM profitable_areas ORDER BY profitability DESC LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a734d2-55d9-4fcb-a5e8-5216c0795aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0e03b-c21a-4b3f-888d-0276a873cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create profit stream steps by calculating start cell, profit, apply 15-min watermark and compute median profit.\n",
    "# profit_stream = cleaned_taxi_stream \\\n",
    "#     .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "#     .filter(\"start_cell_id IS NOT NULL\") \\\n",
    "#     .withColumn(\"profit\", when((col(\"fare_amount\") >= 0) & (col(\"tip_amount\") >= 0), col(\"fare_amount\") + col(\"tip_amount\"))) \\\n",
    "#     .withWatermark(\"dropoff_datetime\", \"15 minutes\") \\\n",
    "#     .groupBy(window(\"dropoff_datetime\", \"15 minutes\"), col(\"start_cell_id\")) \\\n",
    "#     .agg(\n",
    "#         expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"),\n",
    "#         expr(\"first(pickup_datetime)\").alias(\"pickup_datetime\"),\n",
    "#         expr(\"first(dropoff_datetime)\").alias(\"dropoff_datetime\")\n",
    "#     ) \\\n",
    "#     .selectExpr(\"start_cell_id as cell_id\", \"median_profit\", \"window.end as profit_window_end\", \"pickup_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "# profit_stream.writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"profit_stream\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .trigger(processingTime=\"1 seconds\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c3a95-50a2-45bf-823c-ca598bc7f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create empty_taxies_stream by saving pickup table \n",
    "# cleaned_taxi_stream \\\n",
    "#     .select(\"medallion\", \"pickup_datetime\") \\\n",
    "#     .filter(\"medallion IS NOT NULL\") \\\n",
    "#     .withWatermark(\"pickup_datetime\", \"30 minutes\") \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"pickup_table\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .start()\n",
    "\n",
    "# pickup_static = spark.read.table(\"pickup_table\")\n",
    "\n",
    "# # get all drop off in last 30 minutes\n",
    "# dropoffs = cleaned_taxi_stream \\\n",
    "#     .withColumn(\"dropoff_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "#     .filter(\"dropoff_cell_id IS NOT NULL\") \\\n",
    "#     .select(\"medallion\", \"dropoff_datetime\", \"dropoff_cell_id\") \\\n",
    "#     .withWatermark(\"dropoff_datetime\", \"30 minutes\")\n",
    "\n",
    "# # calculate empty taxies based on datetimes\n",
    "# empty_taxis = dropoffs.alias(\"d\").join(\n",
    "#     pickup_static.alias(\"p\"),\n",
    "#     (col(\"d.medallion\") == col(\"p.medallion\")) & (col(\"p.pickup_datetime\") > col(\"d.dropoff_datetime\")),\n",
    "#     how=\"left_anti\"\n",
    "# )\n",
    "\n",
    "# # calculate empty taxes per cell\n",
    "# empty_taxis_per_cell = empty_taxis \\\n",
    "#     .groupBy(window(\"dropoff_datetime\", \"30 minutes\"), col(\"dropoff_cell_id\")) \\\n",
    "#     .agg(expr(\"approx_count_distinct(medallion)\").alias(\"empty_taxis\")) \\\n",
    "#     .selectExpr(\"dropoff_cell_id as cell_id\", \"empty_taxis\", \"window.end as empty_window_end\")\n",
    "\n",
    "# #read to memory\n",
    "# empty_taxis_per_cell.writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"empty_taxis_stream\") \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .trigger(processingTime=\"30 seconds\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177391e6-ad5e-489b-b474-a7d75b8465dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, expr, window\n",
    "\n",
    "# # Prepare the pickup stream\n",
    "# pickup_stream = cleaned_taxi_stream \\\n",
    "#     .select(\"medallion\", \"pickup_datetime\") \\\n",
    "#     .filter(\"medallion IS NOT NULL\") \\\n",
    "#     .withWatermark(\"pickup_datetime\", \"30 minutes\")\n",
    "\n",
    "# # Prepare the dropoff stream with cell ID\n",
    "# dropoff_stream = cleaned_taxi_stream \\\n",
    "#     .withColumn(\"dropoff_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "#     .filter(\"dropoff_cell_id IS NOT NULL\") \\\n",
    "#     .select(\"medallion\", \"dropoff_datetime\", \"dropoff_cell_id\") \\\n",
    "#     .withWatermark(\"dropoff_datetime\", \"30 minutes\")\n",
    "\n",
    "# # Perform stream-stream join to find matching pickups (future pickups after a dropoff)\n",
    "# dropoff_with_future_pickups = dropoff_stream.alias(\"d\").join(\n",
    "#     pickup_stream.alias(\"p\"),\n",
    "#     (col(\"d.medallion\") == col(\"p.medallion\")) &\n",
    "#     (col(\"p.pickup_datetime\") > col(\"d.dropoff_datetime\")) &\n",
    "#     (col(\"p.pickup_datetime\") <= expr(\"d.dropoff_datetime + interval 30 minutes\")),\n",
    "#     \"inner\"\n",
    "# )\n",
    "\n",
    "# # Find dropoffs without future pickups (anti-join)\n",
    "# dropoff_stream_aliased = dropoff_stream.alias(\"d\")\n",
    "# pickup_stream_aliased = pickup_stream.alias(\"p\")\n",
    "\n",
    "# empty_taxis = dropoff_stream_aliased.join(\n",
    "#     pickup_stream_aliased,\n",
    "#     (col(\"d.medallion\") == col(\"p.medallion\")) &\n",
    "#     (col(\"p.pickup_datetime\") > col(\"d.dropoff_datetime\")) &\n",
    "#     (col(\"p.pickup_datetime\") <= expr(\"d.dropoff_datetime + interval 30 minutes\")),\n",
    "#     how=\"leftOuter\"\n",
    "# ).filter(col(\"p.pickup_datetime\").isNull())  # Unambiguous now\n",
    "\n",
    "\n",
    "\n",
    "# # Count distinct empty taxis per dropoff cell in 30-minute windows\n",
    "# empty_taxis_per_cell = empty_taxis \\\n",
    "#     .groupBy(\n",
    "#         window(\"dropoff_datetime\", \"30 minutes\"),\n",
    "#         col(\"dropoff_cell_id\")\n",
    "#     ) \\\n",
    "#     .agg(expr(\"approx_count_distinct(medallion)\").alias(\"empty_taxis\")) \\\n",
    "#     .selectExpr(\n",
    "#         \"dropoff_cell_id as cell_id\",\n",
    "#         \"empty_taxis\",\n",
    "#         \"window.end as empty_window_end\"\n",
    "#     )\n",
    "\n",
    "# # Write to memory for query\n",
    "# empty_taxis_per_cell.writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"empty_taxis_stream\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .trigger(processingTime=\"1 seconds\") \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36648758-ecb5-4cdd-b147-d74cd842032e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # process the patches\n",
    "# def process_batch_part1(_, __): # skip parameters\n",
    "#     spark.sql(\"\"\"\n",
    "#         SELECT \n",
    "#             p.pickup_datetime, \n",
    "#             p.dropoff_datetime, \n",
    "#             p.cell_id AS profitable_cell_id,\n",
    "#             e.empty_taxis AS empty_taxies_in_cell, \n",
    "#             p.median_profit AS median_profit_in_cell,\n",
    "#         CASE \n",
    "#             WHEN e.empty_taxis = 0 THEN NULL \n",
    "#             ELSE p.median_profit / e.empty_taxis \n",
    "#             END AS profitability_of_cell\n",
    "#         FROM profit_stream p\n",
    "#         JOIN empty_taxis_stream e ON p.cell_id = e.cell_id\n",
    "\n",
    "#     \"\"\").show(truncate=False, n=50)\n",
    "\n",
    "# cleaned_taxi_stream.selectExpr(\"CAST(NULL AS STRING) as test\") \\\n",
    "#     .writeStream \\\n",
    "#     .foreachBatch(process_batch_part1) \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/part1_checkpoint\") \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d37435-255c-4ad7-8fd3-229ff7fcc4b6",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933b685-a46a-4610-bf79-14f1dd75424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291a3f8-5ee6-4efc-a48a-a03e24cf9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_profit(df, epoch_id):\n",
    "    if df.isEmpty():\n",
    "        return\n",
    "\n",
    "    import time\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # Get current time in ms\n",
    "    output_time = time.time() * 1000\n",
    "\n",
    "    # Get latest record for delay calculation\n",
    "    latest_event = df.orderBy(col(\"processing_time\").desc()).first()\n",
    "    delay = output_time - latest_event[\"processing_time\"]\n",
    "\n",
    "    # Get top 10 profitable areas\n",
    "    top_areas = spark.sql(\"\"\"\n",
    "        SELECT cell_id, empty_taxis, median_profit, profitability\n",
    "        FROM profitable_areas\n",
    "        ORDER BY profitability DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()\n",
    "\n",
    "    # Construct result row\n",
    "    result = {\n",
    "        \"pickup_datetime\": latest_event[\"pickup_datetime\"],\n",
    "        \"dropoff_datetime\": latest_event[\"dropoff_datetime\"],\n",
    "        \"delay\": delay\n",
    "    }\n",
    "\n",
    "    for i in range(10):\n",
    "        if i < len(top_areas):\n",
    "            result[f\"cell_id_{i+1}\"] = top_areas[i][\"cell_id\"]\n",
    "            result[f\"empty_taxis_in_cell_{i+1}\"] = top_areas[i][\"empty_taxis\"]\n",
    "            result[f\"median_profit_in_cell_{i+1}\"] = top_areas[i][\"median_profit\"]\n",
    "            result[f\"profitability_of_cell_{i+1}\"] = top_areas[i][\"profitability\"]\n",
    "        else:\n",
    "            result[f\"cell_id_{i+1}\"] = None\n",
    "            result[f\"empty_taxis_in_cell_{i+1}\"] = None\n",
    "            result[f\"median_profit_in_cell_{i+1}\"] = None\n",
    "            result[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    # Convert to DataFrame and write to Delta Lake\n",
    "    result_df = spark.createDataFrame([result])\n",
    "    \n",
    "    result_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"/mnt/output/top_profitable_areas_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaf409-d174-40b5-95a9-4dcaf48d6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "profitability_with_time.writeStream \\\n",
    "    .foreachBatch(process_batch_profit) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1483aea-c681-4bcf-bea7-c657465da1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_df = spark.read.format(\"delta\").load(\"/mnt/output/top_profitable_areas_delta\")\n",
    "top10_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf0ccc-a02b-4def-bf5d-ee26d20a903b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
