{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ac151b-3b3a-4238-9a5b-3e5dca122326",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdea92-a01b-4f20-baa7-e5495f1f63c3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab86fdf-de40-4796-b00c-d4da3e65bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from delta import *\n",
    "from pyspark.sql.functions import from_json, col, floor, window, concat_ws\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c90f46d-8d4f-4609-b662-2d9ed30574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession with the Kafka JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTaxiStream\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a407f2b8-4c63-412c-9c90-8a25f05655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema for Incoming Data\n",
    "schema = StructType() \\\n",
    "    .add(\"medallion\", StringType()) \\\n",
    "    .add(\"hack_license\", StringType()) \\\n",
    "    .add(\"pickup_datetime\", TimestampType()) \\\n",
    "    .add(\"dropoff_datetime\", TimestampType()) \\\n",
    "    .add(\"trip_time_in_secs\", DoubleType()) \\\n",
    "    .add(\"trip_distance\", DoubleType()) \\\n",
    "    .add(\"pickup_longitude\", DoubleType()) \\\n",
    "    .add(\"pickup_latitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_longitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_latitude\", DoubleType()) \\\n",
    "    .add(\"payment_type\", StringType()) \\\n",
    "    .add(\"fare_amount\", DoubleType()) \\\n",
    "    .add(\"surcharge\", DoubleType()) \\\n",
    "    .add(\"mta_tax\", DoubleType()) \\\n",
    "    .add(\"tip_amount\", DoubleType()) \\\n",
    "    .add(\"tolls_amount\", DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kafka stream\n",
    "taxi_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"taxi-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "parsed_taxi_stream = taxi_stream.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bc510-4ddc-45ed-aaf1-8a33fd614d03",
   "metadata": {},
   "source": [
    "# Query 0: Data Cleansing and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1dd0a5-68d2-45c0-8842-1768f91c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove malformed and invalid data\n",
    "cleaned_taxi_stream = parsed_taxi_stream \\\n",
    "    .filter(\"medallion IS NOT NULL AND hack_license IS NOT NULL\") \\\n",
    "    .filter(\"pickup_longitude != 0.0 AND pickup_latitude != 0.0\") \\\n",
    "    .filter(\"dropoff_longitude != 0.0 AND dropoff_latitude != 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5968d8ef-d0e3-420d-9f67-256e35d05ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cleaned_taxi_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"taxi_trips_cleaned\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4abc85-cefc-40c3-a1d6-95f6ba11ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|medallion|hack_license|pickup_datetime|dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View cleansed stream in notebook\n",
    "time.sleep(1)\n",
    "spark.sql(\"SELECT * FROM taxi_trips_cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555b6a1-5a5c-4d2c-b392-79977f2b3c89",
   "metadata": {},
   "source": [
    "# Query 1: Frequent Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f88189-8394-463e-9114-c6aad07b6c75",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37e0f61-17e8-4e07-a543-fec9aa875975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ba2a62-05a0-46d4-9617-a9876b8e08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_lat = 41.474937\n",
    "reference_lon = -74.913585\n",
    "total_cells = 300\n",
    "\n",
    "# Cell sizes from http://www.debs2015.org/call-grand-challenge.html\n",
    "cell_size_lat_deg = 0.004491556  # 500m south\n",
    "cell_size_lon_deg = 0.005986     # 500m east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cc6b1a-4ad4-4e4d-a7c8-caf0d2c6a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to calculate how far the cell location is from the origin\n",
    "def get_cell_id(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    try:\n",
    "        dx = int((lon - reference_lon) / cell_size_lon_deg) + 1 # how many cells east\n",
    "        dy = int((reference_lat - lat) / cell_size_lat_deg) + 1 # how many cells south\n",
    "        if 1 <= dx <= total_cells and 1 <= dy <= total_cells: # validate\n",
    "            return f\"{dx}.{dy}\"\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# create spark udf\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c843e763-11a8-4e28-80b7-9da7334fcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take cleaned taxi stream data and convert pickup and dropoff locations into start and end cell IDs\n",
    "stream_with_cells = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"end_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL AND end_cell_id IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ea44ef-8517-4a7f-842b-266c1768f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rides for each route in the last 30 minutes \n",
    "frequent_routes = stream_with_cells \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"30 minutes\"),\n",
    "        col(\"start_cell_id\"),\n",
    "        col(\"end_cell_id\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\"start_cell_id\", \"end_cell_id\", \"count\") \\\n",
    "    .orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3219b4-8d30-4041-83df-2e24cb3df4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f89e87116d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store results\n",
    "frequent_routes.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"top_routes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01aa8050-4b84-42e4-ba22-467f860def02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+\n",
      "|start_cell_id|end_cell_id|count|\n",
      "+-------------+-----------+-----+\n",
      "+-------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time.sleep(15)\n",
    "spark.sql(\"SELECT * FROM top_routes LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9299a-ce45-455c-a4a6-311f47ac8c48",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138b680-2314-4517-8694-9e2013bf7205",
   "metadata": {},
   "source": [
    "# Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71532313-5144-4e9f-b28d-8c55da28057d",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52959a9c-7791-4604-a655-bcf5cc28e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import window, max as max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3425dc-2fc8-4b67-977f-3d5816f61051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide query 1 degrees by 2\n",
    "cell_size_lat_deg = 0.002245778   # 250m south\n",
    "cell_size_lon_deg = 0.002993      # 250m east\n",
    "total_cells = 600\n",
    "\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "030a46df-15af-4c2f-becb-2a761948e5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f89d9f509d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create profit stream steps by calculating start cell, profit, apply 15-min watermark and compute median profit.\n",
    "profit_stream = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL\") \\\n",
    "    .withColumn(\"profit\", when((col(\"fare_amount\") >= 0) & (col(\"tip_amount\") >= 0), col(\"fare_amount\") + col(\"tip_amount\"))) \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"15 minutes\") \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"15 minutes\"), col(\"start_cell_id\")) \\\n",
    "    .agg(\n",
    "        expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"),\n",
    "        max_(\"pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "        max_(\"dropoff_datetime\").alias(\"dropoff_datetime\")\n",
    "    ) \\\n",
    "    .selectExpr(\"start_cell_id as cell_id\", \"median_profit\", \"window.end as profit_window_end\", \"pickup_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "# Write to Delta with partitioning by cell_id\n",
    "profit_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .queryName(\"profit_stream\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"cell_id\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .option(\"checkpointLocation\", \"./checkpoints/profit_stream\") \\\n",
    "    .option(\"path\", \"./output/profitable_areas_delta\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de900cf3-0650-406d-9caa-704bbbcb792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f89d969e250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty_taxies_stream by saving pickup table \n",
    "cleaned_taxi_stream \\\n",
    "    .select(\"medallion\", \"pickup_datetime\") \\\n",
    "    .filter(\"medallion IS NOT NULL\") \\\n",
    "    .withWatermark(\"pickup_datetime\", \"30 minutes\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"pickup_table\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "pickup_static = spark.read.table(\"pickup_table\")\n",
    "\n",
    "# get all drop off in last 30 minutes\n",
    "dropoffs = cleaned_taxi_stream \\\n",
    "    .withColumn(\"dropoff_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"dropoff_cell_id IS NOT NULL\") \\\n",
    "    .select(\"medallion\", \"dropoff_datetime\", \"dropoff_cell_id\") \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\")\n",
    "\n",
    "# Calculate empty taxis based on datetimes\n",
    "empty_taxis = dropoffs.alias(\"d\").join(\n",
    "    pickup_static.alias(\"p\"),\n",
    "    (col(\"d.medallion\") == col(\"p.medallion\")) & (col(\"p.pickup_datetime\") > col(\"d.dropoff_datetime\")),\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Calculate empty taxis per cell\n",
    "empty_taxis_per_cell = empty_taxis \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"30 minutes\"), col(\"dropoff_cell_id\")) \\\n",
    "    .agg(expr(\"approx_count_distinct(medallion)\").alias(\"empty_taxis\")) \\\n",
    "    .selectExpr(\"dropoff_cell_id as cell_id\", \"empty_taxis\", \"window.end as empty_window_end\")\n",
    "\n",
    "# Write empty taxis to memory\n",
    "empty_taxis_per_cell.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"empty_taxis_stream\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00fdef48-2491-459f-8b15-60df62595532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f89c8223610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join together and process batches\n",
    "def process_batch(batch_df, batch_id):\n",
    "    result_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.pickup_datetime, p.dropoff_datetime, p.cell_id AS profitable_cell_id,\n",
    "            e.empty_taxis AS empty_taxies_in_cell, \n",
    "            p.median_profit AS median_profit_in_cell,\n",
    "            CASE \n",
    "                WHEN e.empty_taxis = 0 THEN NULL \n",
    "                ELSE p.median_profit / e.empty_taxis \n",
    "            END AS profitability_of_cell\n",
    "        FROM profit_stream p\n",
    "        JOIN empty_taxis_stream e ON p.cell_id = e.cell_id\n",
    "    \"\"\")\n",
    "    result_df.show(truncate=False, n=50)\n",
    "\n",
    "    # Write the results to Delta or Parquet\n",
    "    result_df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(\"./output/profitable_areas_parquet\")\n",
    "\n",
    "# Write processed batch to Delta or Parquet\n",
    "cleaned_taxi_stream.selectExpr(\"CAST(NULL AS STRING) as test\") \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09d76f1d-a8cf-4496-a0d3-89287cdb7b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|2013-01-01 00:18:00|2013-01-01 00:22:00|316.324           |2                   |7.0                  |3.5                  |\n",
      "|2013-01-01 00:26:00|2013-01-01 00:29:00|321.314           |24                  |10.5                 |0.4375               |\n",
      "|2013-01-01 00:14:00|2013-01-01 00:27:00|314.304           |10                  |13.0                 |1.3                  |\n",
      "|2013-01-01 00:12:39|2013-01-01 00:14:33|312.310           |15                  |5.5                  |0.36666666666666664  |\n",
      "|2013-01-01 00:19:00|2013-01-01 00:28:00|303.344           |3                   |7.5                  |2.5                  |\n",
      "|2013-01-01 00:06:26|2013-01-01 00:14:37|303.327           |16                  |6.5                  |0.40625              |\n",
      "|2013-01-01 00:20:13|2013-01-01 00:29:00|312.311           |13                  |8.5                  |0.6538461538461539   |\n",
      "|2013-01-01 00:16:02|2013-01-01 00:24:01|322.334           |1                   |7.5                  |7.5                  |\n",
      "|2013-01-01 00:09:15|2013-01-01 00:11:01|321.315           |19                  |4.0                  |0.21052631578947367  |\n",
      "|2013-01-01 00:19:00|2013-01-01 00:23:00|303.342           |7                   |5.0                  |0.7142857142857143   |\n",
      "|2013-01-01 00:23:00|2013-01-01 00:29:00|322.309           |14                  |12.1                 |0.8642857142857142   |\n",
      "|2013-01-01 00:26:04|2013-01-01 00:28:58|306.325           |6                   |8.5                  |1.4166666666666667   |\n",
      "|2013-01-01 00:09:35|2013-01-01 00:14:19|316.317           |8                   |4.5                  |0.5625               |\n",
      "|2013-01-01 00:09:13|2013-01-01 00:13:46|311.333           |19                  |7.5                  |0.39473684210526316  |\n",
      "|2013-01-01 00:23:00|2013-01-01 00:29:00|313.327           |12                  |7.5                  |0.625                |\n",
      "|2013-01-01 00:21:00|2013-01-01 00:28:15|308.323           |13                  |11.0                 |0.8461538461538461   |\n",
      "|2013-01-01 00:07:00|2013-01-01 00:19:00|308.316           |3                   |11.5                 |3.8333333333333335   |\n",
      "|2013-01-01 00:13:00|2013-01-01 00:23:00|321.356           |1                   |10.9                 |10.9                 |\n",
      "|2013-01-01 00:20:55|2013-01-01 00:26:53|316.315           |9                   |6.5                  |0.7222222222222222   |\n",
      "|2013-01-01 00:05:56|2013-01-01 00:09:26|315.325           |15                  |6.0                  |0.4                  |\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Read the Delta table that was written\n",
    "# df = spark.read.format(\"delta\").load(\"./output/profitable_areas_delta\")\n",
    "\n",
    "# # Show the top rows of the Delta data\n",
    "# df.show(truncate=False)\n",
    "\n",
    "# Read the Parquet file\n",
    "df = spark.read.parquet(\"./output/profitable_areas_parquet\")\n",
    "\n",
    "# Show the data\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d37435-255c-4ad7-8fd3-229ff7fcc4b6",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0933b685-a46a-4610-bf79-14f1dd75424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8291a3f8-5ee6-4efc-a48a-a03e24cf9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_profit(df, epoch_id):\n",
    "    if df.isEmpty():\n",
    "        return\n",
    "\n",
    "    import time\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # Get current time in ms\n",
    "    output_time = time.time() * 1000\n",
    "\n",
    "    # Get latest record for delay calculation\n",
    "    latest_event = df.orderBy(col(\"processing_time\").desc()).first()\n",
    "    delay = output_time - latest_event[\"processing_time\"]\n",
    "\n",
    "    # Get top 10 profitable areas\n",
    "    top_areas = spark.sql(\"\"\"\n",
    "        SELECT cell_id, empty_taxis, median_profit, profitability\n",
    "        FROM profitable_areas\n",
    "        ORDER BY profitability DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()\n",
    "\n",
    "    # Construct result row\n",
    "    result = {\n",
    "        \"pickup_datetime\": latest_event[\"pickup_datetime\"],\n",
    "        \"dropoff_datetime\": latest_event[\"dropoff_datetime\"],\n",
    "        \"delay\": delay\n",
    "    }\n",
    "\n",
    "    for i in range(10):\n",
    "        if i < len(top_areas):\n",
    "            result[f\"cell_id_{i+1}\"] = top_areas[i][\"cell_id\"]\n",
    "            result[f\"empty_taxis_in_cell_{i+1}\"] = top_areas[i][\"empty_taxis\"]\n",
    "            result[f\"median_profit_in_cell_{i+1}\"] = top_areas[i][\"median_profit\"]\n",
    "            result[f\"profitability_of_cell_{i+1}\"] = top_areas[i][\"profitability\"]\n",
    "        else:\n",
    "            result[f\"cell_id_{i+1}\"] = None\n",
    "            result[f\"empty_taxis_in_cell_{i+1}\"] = None\n",
    "            result[f\"median_profit_in_cell_{i+1}\"] = None\n",
    "            result[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    # Convert to DataFrame and write to Delta Lake\n",
    "    result_df = spark.createDataFrame([result])\n",
    "    \n",
    "    result_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"./output/top_profitable_areas_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bafaf409-d174-40b5-95a9-4dcaf48d6337",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profitability_with_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprofitability_with_time\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(process_batch_profit) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 second\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profitability_with_time' is not defined"
     ]
    }
   ],
   "source": [
    "profitability_with_time.writeStream \\\n",
    "    .foreachBatch(process_batch_profit) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"5 second\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1483aea-c681-4bcf-bea7-c657465da1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_df = spark.read.format(\"delta\").load(\"./output/top_profitable_areas_delta\")\n",
    "top10_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf0ccc-a02b-4def-bf5d-ee26d20a903b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
