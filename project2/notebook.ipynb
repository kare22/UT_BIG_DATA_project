{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ac151b-3b3a-4238-9a5b-3e5dca122326",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdea92-a01b-4f20-baa7-e5495f1f63c3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab86fdf-de40-4796-b00c-d4da3e65bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, floor, window, concat_ws\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c90f46d-8d4f-4609-b662-2d9ed30574af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 56070)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession with the Kafka JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTaxiStream\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark Session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a407f2b8-4c63-412c-9c90-8a25f05655e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema for Incoming Data\n",
    "schema = StructType() \\\n",
    "    .add(\"medallion\", StringType()) \\\n",
    "    .add(\"hack_license\", StringType()) \\\n",
    "    .add(\"pickup_datetime\", TimestampType()) \\\n",
    "    .add(\"dropoff_datetime\", TimestampType()) \\\n",
    "    .add(\"trip_time_in_secs\", DoubleType()) \\\n",
    "    .add(\"trip_distance\", DoubleType()) \\\n",
    "    .add(\"pickup_longitude\", DoubleType()) \\\n",
    "    .add(\"pickup_latitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_longitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_latitude\", DoubleType()) \\\n",
    "    .add(\"payment_type\", StringType()) \\\n",
    "    .add(\"fare_amount\", DoubleType()) \\\n",
    "    .add(\"surcharge\", DoubleType()) \\\n",
    "    .add(\"mta_tax\", DoubleType()) \\\n",
    "    .add(\"tip_amount\", DoubleType()) \\\n",
    "    .add(\"tolls_amount\", DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kafka stream\n",
    "taxi_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"taxi-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "parsed_taxi_stream = taxi_stream.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bc510-4ddc-45ed-aaf1-8a33fd614d03",
   "metadata": {},
   "source": [
    "# Query 0: Data Cleansing and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1dd0a5-68d2-45c0-8842-1768f91c9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove malformed and invalid data\n",
    "cleaned_taxi_stream = parsed_taxi_stream \\\n",
    "    .filter(\"medallion IS NOT NULL AND hack_license IS NOT NULL\") \\\n",
    "    .filter(\"pickup_longitude != 0.0 AND pickup_latitude != 0.0\") \\\n",
    "    .filter(\"dropoff_longitude != 0.0 AND dropoff_latitude != 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5968d8ef-d0e3-420d-9f67-256e35d05ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cleaned_taxi_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"taxi_trips_cleaned\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4abc85-cefc-40c3-a1d6-95f6ba11ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "|medallion|hack_license|pickup_datetime|dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "+---------+------------+---------------+----------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View cleansed stream in notebook\n",
    "spark.sql(\"SELECT * FROM taxi_trips_cleaned\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555b6a1-5a5c-4d2c-b392-79977f2b3c89",
   "metadata": {},
   "source": [
    "# Query 1: Frequent Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f88189-8394-463e-9114-c6aad07b6c75",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37e0f61-17e8-4e07-a543-fec9aa875975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ba2a62-05a0-46d4-9617-a9876b8e08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_lat = 41.474937\n",
    "reference_lon = -74.913585\n",
    "total_cells = 300\n",
    "\n",
    "# Cell sizes\n",
    "cell_size_lat_deg = 0.004491556  # 500m south\n",
    "cell_size_lon_deg = 0.005986     # 500m east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cc6b1a-4ad4-4e4d-a7c8-caf0d2c6a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to calculate how far the location is from the origin\n",
    "def get_cell_id(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    try:\n",
    "        dx = int((lon - reference_lon) / cell_size_lon_deg) + 1 # how many cells east\n",
    "        dy = int((reference_lat - lat) / cell_size_lat_deg) + 1 # how many cells south\n",
    "        if 1 <= dx <= total_cells and 1 <= dy <= total_cells: # validate\n",
    "            return f\"{dx}.{dy}\"\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# create spark udf\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c843e763-11a8-4e28-80b7-9da7334fcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take cleaned taxi stream data and convert pickup and dropoff locations into start and end cell IDs\n",
    "stream_with_cells = cleaned_taxi_stream \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"end_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL AND end_cell_id IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ea44ef-8517-4a7f-842b-266c1768f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rides for each route in the last 30 minutes \n",
    "frequent_routes = stream_with_cells \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"30 minutes\"),\n",
    "        col(\"start_cell_id\"),\n",
    "        col(\"end_cell_id\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\"start_cell_id\", \"end_cell_id\", \"count\") \\\n",
    "    .orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad3219b4-8d30-4041-83df-2e24cb3df4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f57d85216d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store results\n",
    "frequent_routes.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"top_routes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01aa8050-4b84-42e4-ba22-467f860def02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+\n",
      "|start_cell_id|end_cell_id|count|\n",
      "+-------------+-----------+-----+\n",
      "+-------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM top_routes LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9299a-ce45-455c-a4a6-311f47ac8c48",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138b680-2314-4517-8694-9e2013bf7205",
   "metadata": {},
   "source": [
    "# Query 2: Profitable Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71532313-5144-4e9f-b28d-8c55da28057d",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52959a9c-7791-4604-a655-bcf5cc28e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import window, max as max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3425dc-2fc8-4b67-977f-3d5816f61051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide last query 1 degrees by 2\n",
    "cell_size_lat_deg = 0.002245778   # 250m south\n",
    "cell_size_lon_deg = 0.002993      # 250m east\n",
    "total_cells = 600\n",
    "\n",
    "get_cell_udf = udf(get_cell_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d0e03b-c21a-4b3f-888d-0276a873cce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f57d0308a50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profitAgg = cleaned_taxi_stream \\\n",
    "    .filter(\"fare_amount >= 0 AND tip_amount >= 0\") \\\n",
    "    .withColumn(\"start_cell_id\", get_cell_udf(\"pickup_latitude\", \"pickup_longitude\")) \\\n",
    "    .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\")) \\\n",
    "    .filter(\"start_cell_id IS NOT NULL\") \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"15 minutes\") \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"15 minutes\"), col(\"start_cell_id\")) \\\n",
    "    .agg(\n",
    "        expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"),\n",
    "        max_(\"pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "        max_(\"dropoff_datetime\").alias(\"dropoff_datetime\")\n",
    "    ) \\\n",
    "    .selectExpr(\"start_cell_id as cell_id\", \n",
    "                \"median_profit\", \n",
    "                \"window.end as profit_window_end\", \n",
    "                \"pickup_datetime\", \n",
    "                \"dropoff_datetime\")\n",
    "\n",
    "profitAgg.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"profitAgg\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "441c3a95-50a2-45bf-823c-ca598bc7f730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f57d030b590>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emptyAgg = cleaned_taxi_stream \\\n",
    "    .withColumn(\"dropoff_cell_id\", get_cell_udf(\"dropoff_latitude\", \"dropoff_longitude\")) \\\n",
    "    .filter(\"dropoff_cell_id IS NOT NULL\") \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\") \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"30 minutes\"), col(\"dropoff_cell_id\")) \\\n",
    "    .agg(expr(\"approx_count_distinct(medallion)\").alias(\"empty_taxis\")) \\\n",
    "    .selectExpr(\"dropoff_cell_id as cell_id\", \"empty_taxis\", \"window.end as empty_window_end\")\n",
    "\n",
    "emptyAgg.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"emptyAgg\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36648758-ecb5-4cdd-b147-d74cd842032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f57d813efd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime|dropoff_datetime|profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+---------------+----------------+------------------+--------------------+---------------------+---------------------+\n",
      "+---------------+----------------+------------------+--------------------+---------------------+---------------------+\n",
      "\n",
      "+---------------+----------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime|dropoff_datetime|profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+---------------+----------------+------------------+--------------------+---------------------+---------------------+\n",
      "+---------------+----------------+------------------+--------------------+---------------------+---------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|2013-01-01 00:24:00|2013-01-01 00:25:00|296.326           |1                   |72.0                 |72.0                 |\n",
      "|2013-01-01 00:11:00|2013-01-01 00:11:00|296.314           |1                   |63.0                 |63.0                 |\n",
      "|2013-01-01 00:11:30|2013-01-01 00:12:37|293.337           |1                   |49.65                |49.65                |\n",
      "|2013-01-01 00:10:46|2013-01-01 00:27:35|379.370           |1                   |41.5                 |41.5                 |\n",
      "|2013-01-01 00:06:17|2013-01-01 00:06:57|338.277           |1                   |37.5                 |37.5                 |\n",
      "|2013-01-01 00:08:17|2013-01-01 00:27:13|308.351           |1                   |32.0                 |32.0                 |\n",
      "|2013-01-01 00:07:06|2013-01-01 00:24:40|325.306           |1                   |30.0                 |30.0                 |\n",
      "|2013-01-01 00:03:00|2013-01-01 00:26:00|330.315           |1                   |28.3                 |28.3                 |\n",
      "|2013-01-01 00:05:00|2013-01-01 00:25:00|314.335           |1                   |25.5                 |25.5                 |\n",
      "|2013-01-01 00:08:19|2013-01-01 00:23:15|311.360           |1                   |25.4                 |25.4                 |\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|2013-01-01 00:24:00|2013-01-01 00:25:00|296.326           |1                   |72.0                 |72.0                 |\n",
      "|2013-01-01 00:11:00|2013-01-01 00:11:00|296.314           |1                   |63.0                 |63.0                 |\n",
      "|2013-01-01 00:11:30|2013-01-01 00:12:37|293.337           |1                   |49.65                |49.65                |\n",
      "|2013-01-01 00:10:46|2013-01-01 00:27:35|379.370           |1                   |41.5                 |41.5                 |\n",
      "|2013-01-01 00:06:17|2013-01-01 00:06:57|338.277           |1                   |37.5                 |37.5                 |\n",
      "|2013-01-01 00:08:17|2013-01-01 00:27:13|308.351           |1                   |32.0                 |32.0                 |\n",
      "|2013-01-01 00:07:06|2013-01-01 00:24:40|325.306           |1                   |30.0                 |30.0                 |\n",
      "|2013-01-01 00:03:00|2013-01-01 00:26:00|330.315           |1                   |28.3                 |28.3                 |\n",
      "|2013-01-01 00:05:00|2013-01-01 00:25:00|314.335           |1                   |25.5                 |25.5                 |\n",
      "|2013-01-01 00:08:19|2013-01-01 00:23:15|311.360           |1                   |25.4                 |25.4                 |\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|2013-01-01 00:24:00|2013-01-01 00:25:00|296.326           |1                   |72.0                 |72.0                 |\n",
      "|2013-01-01 00:11:00|2013-01-01 00:11:00|296.314           |1                   |63.0                 |63.0                 |\n",
      "|2013-01-01 00:11:30|2013-01-01 00:12:37|293.337           |1                   |49.65                |49.65                |\n",
      "|2013-01-01 00:10:46|2013-01-01 00:27:35|379.370           |1                   |41.5                 |41.5                 |\n",
      "|2013-01-01 00:06:17|2013-01-01 00:06:57|338.277           |1                   |37.5                 |37.5                 |\n",
      "|2013-01-01 00:08:17|2013-01-01 00:27:13|308.351           |1                   |32.0                 |32.0                 |\n",
      "|2013-01-01 00:07:06|2013-01-01 00:24:40|325.306           |1                   |30.0                 |30.0                 |\n",
      "|2013-01-01 00:03:00|2013-01-01 00:26:00|330.315           |1                   |28.3                 |28.3                 |\n",
      "|2013-01-01 00:05:00|2013-01-01 00:25:00|314.335           |1                   |25.5                 |25.5                 |\n",
      "|2013-01-01 00:08:19|2013-01-01 00:23:15|311.360           |1                   |25.4                 |25.4                 |\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id|empty_taxies_in_cell|median_profit_in_cell|profitability_of_cell|\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "|2013-01-01 00:24:00|2013-01-01 00:25:00|296.326           |1                   |72.0                 |72.0                 |\n",
      "|2013-01-01 00:11:00|2013-01-01 00:11:00|296.314           |1                   |63.0                 |63.0                 |\n",
      "|2013-01-01 00:11:30|2013-01-01 00:12:37|293.337           |1                   |49.65                |49.65                |\n",
      "|2013-01-01 00:10:46|2013-01-01 00:27:35|379.370           |1                   |41.5                 |41.5                 |\n",
      "|2013-01-01 00:06:17|2013-01-01 00:06:57|338.277           |1                   |37.5                 |37.5                 |\n",
      "|2013-01-01 00:08:17|2013-01-01 00:27:13|308.351           |1                   |32.0                 |32.0                 |\n",
      "|2013-01-01 00:07:06|2013-01-01 00:24:40|325.306           |1                   |30.0                 |30.0                 |\n",
      "|2013-01-01 00:03:00|2013-01-01 00:26:00|330.315           |1                   |28.3                 |28.3                 |\n",
      "|2013-01-01 00:05:00|2013-01-01 00:25:00|314.335           |1                   |25.5                 |25.5                 |\n",
      "|2013-01-01 00:08:19|2013-01-01 00:23:15|311.360           |1                   |25.4                 |25.4                 |\n",
      "+-------------------+-------------------+------------------+--------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_batch(_, __):\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.pickup_datetime, p.dropoff_datetime, p.cell_id AS profitable_cell_id,\n",
    "            e.empty_taxis AS empty_taxies_in_cell, p.median_profit AS median_profit_in_cell,\n",
    "            CASE WHEN e.empty_taxis = 0 THEN NULL ELSE p.median_profit / e.empty_taxis END AS profitability_of_cell\n",
    "        FROM profitAgg p\n",
    "        JOIN emptyAgg e ON p.cell_id = e.cell_id\n",
    "        ORDER BY profitability_of_cell DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=False, n=50)\n",
    "\n",
    "cleaned_taxi_stream.selectExpr(\"CAST(NULL AS STRING) as dummy\") \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
